{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, concatenate\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-define about column info\n",
    "COLUMNS = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\", \"marital_status\",\n",
    "          \"occupation\", \"relationship\", \"race\", \"gender\", \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\", \"income_bracket\"]\n",
    "LABEL_COLUMN = \"label\"\n",
    "CATEGORICAL_COLUMNS = [\"workclass\", \"education\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"gender\", \"native_country\"]\n",
    "NUMERICAL_COLUMNS = [\"age\", \"education_num\", \"capital_gain\", \"capital_loss\", \"hours_per_week\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep:\n",
    "    def __init__(self, args):\n",
    "        self.learning_rate = args.learning_rate\n",
    "        self.epochs = args.epochs\n",
    "        self.batch_size = args.batch_size\n",
    "        self.input_dim = args.input_dim\n",
    "        self.model = self.classifier()\n",
    "\n",
    "    def classifier(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(100, activation='relu', input_dim=self.input_dim))\n",
    "        model.add(Dense(50, activation='relu'))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        optimizer = Adam(lr=self.learning_rate, beta_1=0.9, beta_2=0.999)\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer=optimizer,\n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self.model.fit(x, y, epochs=self.epochs, batch_size=self.batch_size, validation_split=0.2)\n",
    "\n",
    "    def print_performance(self, x, y):\n",
    "        performance_test = self.model.evaluate(x, y, batch_size=self.batch_size)\n",
    "        print('Test Loss and Accuracy ->', performance_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wide:\n",
    "    def __init__(self, args):\n",
    "        self.learning_rate = args.learning_rate\n",
    "        self.epochs = args.epochs\n",
    "        self.batch_size = args.batch_size\n",
    "        self.input_dim = args.input_dim\n",
    "        self.model = self.classifier()\n",
    "\n",
    "    def classifier(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(1, activation='sigmoid', input_dim=self.input_dim))\n",
    "\n",
    "        optimizer = Adam(lr=self.learning_rate, beta_1=0.9, beta_2=0.999)\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer=optimizer,\n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self.model.fit(x, y, epochs=self.epochs, batch_size=self.batch_size, validation_split=0.2)\n",
    "\n",
    "    def print_performance(self, x, y):\n",
    "        performance_test = self.model.evaluate(x, y, batch_size=self.batch_size)\n",
    "        print('Test Loss and Accuracy ->', performance_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeep:\n",
    "    def __init__(self, args):\n",
    "        self.learning_rate = args.learning_rate\n",
    "        self.epochs = args.epochs\n",
    "        self.batch_size = args.batch_size\n",
    "        self.input_dim = args.input_dim\n",
    "        self.model = self.classifier()\n",
    "\n",
    "    def classifier(self):\n",
    "        optimizer = Adam(lr=self.learning_rate, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "        # wide part\n",
    "        wide = Input(shape=(self.input_dim,))\n",
    "\n",
    "        # deep part\n",
    "        deep_input = Input(shape=(self.input_dim,))\n",
    "        deep = Dense(100, activation='relu')(deep_input)\n",
    "        deep = Dense(50, activation='relu')(deep)\n",
    "\n",
    "        # concatenate : wide and deep\n",
    "        wide_n_deep = concatenate([wide, deep])\n",
    "        wide_n_deep = Dense(1, activation='sigmoid')(wide_n_deep)\n",
    "        model = Model(inputs=[wide, deep_input], outputs=wide_n_deep)\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer=optimizer,\n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def fit(self, wide_x, deep_x, y):\n",
    "        self.model.fit([wide_x, deep_x], y, epochs=self.epochs, batch_size=self.batch_size, validation_split=0.2)\n",
    "\n",
    "    def print_performance(self, wide_x, deep_x, y):\n",
    "        performance_test = self.model.evaluate([wide_x, deep_x], y, batch_size=self.batch_size)\n",
    "        print('Test Loss and Accuracy ->', performance_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model_param):\n",
    "    # prepare dataset\n",
    "    df_train = pd.read_csv('http://mlr.cs.umass.edu/ml/machine-learning-databases/adult/adult.data',\n",
    "                           header=None,\n",
    "                           names=COLUMNS,\n",
    "                           skipinitialspace=True)\n",
    "    df_test = pd.read_csv('http://mlr.cs.umass.edu/ml/machine-learning-databases/adult/adult.test',\n",
    "                          header=None,\n",
    "                          names=COLUMNS,\n",
    "                          skipinitialspace=True,\n",
    "                          skiprows=1)\n",
    "    df = pd.concat([df_train, df_test])\n",
    "    df[LABEL_COLUMN] = df['income_bracket'].apply(lambda x: \">50K\" in x).astype(int)\n",
    "    y = df[LABEL_COLUMN].values\n",
    "    df.pop(LABEL_COLUMN)\n",
    "    df.pop(\"income_bracket\")\n",
    "    df = pd.get_dummies(df, columns=[x for x in CATEGORICAL_COLUMNS])\n",
    "    df = pd.DataFrame(MinMaxScaler().fit_transform(df), columns=df.columns)\n",
    "    x = df.values\n",
    "\n",
    "    # split train, test\n",
    "    train_length = len(df_train)\n",
    "    x_train = x[:train_length]\n",
    "    y_train = y[:train_length]\n",
    "    x_test = x[train_length:]\n",
    "    y_test = y[train_length:]\n",
    "\n",
    "    # prepare hyper parameter\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--batch_size', type=int, default=500,\n",
    "                        help='Batch size for networks')\n",
    "    parser.add_argument('--epochs', type=int, default=30,\n",
    "                        help='Epochs for the networks')\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.001,\n",
    "                        help='Learning rate')\n",
    "    parser.add_argument('--input_dim', type=int, default=x_train.shape[1],\n",
    "                        help='Input dimension for the generator.')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if model_param == \"deep\":\n",
    "        deep = Deep(args)\n",
    "        deep.fit(x_train, y_train)\n",
    "        deep.print_performance(x_test, y_test)\n",
    "    elif model_param == 'wide':\n",
    "        wide = Wide(args)\n",
    "        wide.fit(x_train, y_train)\n",
    "        wide.print_performance(x_test, y_test)\n",
    "    else:\n",
    "        wide_n_deep = WideAndDeep(args)\n",
    "        wide_n_deep.fit(x_train, x_train, y_train)\n",
    "        wide_n_deep.print_performance(x_test, x_test, y_test)\n",
    "\n",
    "        # prediction for individual\n",
    "        x_predict_test = x_test[np.newaxis, 0, :]\n",
    "        y_predict_test = y_test[0]\n",
    "        result = wide_n_deep.model.predict([x_predict_test, x_predict_test])\n",
    "        print('result predicted:', result[0][0])\n",
    "        print('result predicted:', round(result[0][0]))\n",
    "        print('result real:', y_predict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\kwak\\doc\\git\\nlp\\venv\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype uint8, int64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "usage: ipykernel_launcher.py [-h] [--batch_size BATCH_SIZE] [--epochs EPOCHS]\n",
      "                             [--learning_rate LEARNING_RATE]\n",
      "                             [--input_dim INPUT_DIM]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\david\\AppData\\Roaming\\jupyter\\runtime\\kernel-94d209aa-898e-47af-b263-06136b393d77.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\kwak\\doc\\git\\nlp\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3304: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main('widendeep')\n",
    "\n",
    "    # score (test loss and acc)\n",
    "    # 1. wide : [0.34640224496809097, 0.8394447512213981]\n",
    "    # 2. deep : [0.3533135050656381, 0.8403660668860226]\n",
    "    # 3. wide and deep : [0.352460421507851, 0.8432528705885703]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
